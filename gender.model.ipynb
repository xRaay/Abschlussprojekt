{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''*Gender* - MODEL BASED IMPUTATION'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary libraries\n",
    "import pandas as pd #data manipulation and analysis\n",
    "from sklearn.model_selection import train_test_split #splitting the data into training and testing sets\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder #scaling the data, encoding the data\n",
    "from sklearn.impute import SimpleImputer #handling missing values\n",
    "from tensorflow.keras.models import Sequential #creating the model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input #importing the layers\n",
    "from tensorflow.keras.optimizers import Adam #the optimizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping #early stop the training\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC, Accuracy, F1Score #metrics\n",
    "from tensorflow.keras import regularizers #aids in preventing overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"C:\\Users\\D\\OneDrive - Grunenthal Group\\Desktop\\VSC - Py\\GRTend\\Models\\gender_model.keras\"\n",
    "\n",
    "final_dataset_path = r\"C:\\Users\\D\\OneDrive - Grunenthal Group\\Desktop\\VSC - Py\\GRTend\\Data\\dataset_final.csv\"\n",
    "\n",
    "gender_path = r\"C:\\Users\\D\\OneDrive - Grunenthal Group\\Desktop\\VSC - Py\\GRTend\\Data\\gender_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(gender_path)\n",
    "\n",
    "df = df.dropna(subset=['Gender'])\n",
    "\n",
    "df.info()\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df.drop('Gender', axis = 1)\n",
    "y = df['Gender']\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical and categorical columns\n",
    "num_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "cat_cols = X_train.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in numerical columns\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train[num_cols] = imputer.fit_transform(X_train[num_cols])\n",
    "X_test[num_cols] = imputer.transform(X_test[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical columns\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoded_cols_train = encoder.fit_transform(X_train[cat_cols])\n",
    "encoded_cols_test = encoder.transform(X_test[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the numpy arrays to pandas DataFrames and reset the index\n",
    "encoded_cols_train = pd.DataFrame(encoded_cols_train, columns=encoder.get_feature_names_out(cat_cols)).reset_index(drop=True)\n",
    "encoded_cols_test = pd.DataFrame(encoded_cols_test, columns=encoder.get_feature_names_out(cat_cols)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of X_train and X_test\n",
    "X_train = X_train[num_cols].reset_index(drop=True)\n",
    "X_test = X_test[num_cols].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate numerical and encoded categorical columns\n",
    "X_train = pd.concat([X_train[num_cols], encoded_cols_train], axis=1)\n",
    "X_test = pd.concat([X_test[num_cols], encoded_cols_test], axis=1)\n",
    "\n",
    "X_train.shape\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.002)), #was 0.001\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.002)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.002)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.002)),\n",
    "    Dropout(0.4),\n",
    "    Dense(1, activation='tanh')\n",
    "])\n",
    "\n",
    "'''\n",
    "Rectified Linear Activation (ReLU)\n",
    "Logistic (Sigmoid)\n",
    "Hyperbolic Tangent (Tanh)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.001, beta_1=0.90, beta_2=0.999, epsilon=1e-07, amsgrad=True),\n",
    "    metrics=['accuracy', 'precision', 'recall', 'auc']\n",
    ")\n",
    "\n",
    "\n",
    "'''\n",
    "Adam = Adaptive Moment Estimation, instead of Stochastic Gradient Descent\n",
    "Learning rate = rate at which the model learns\n",
    "Beta 1 = exponential decay rate for the first moment estimates\n",
    "Beta 2 = exponential decay rate for the second moment estimates\n",
    "Epsilon = small number to prevent division by zero\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping\n",
    "early_stopping_loss = EarlyStopping(monitor='val_loss', patience=369, restore_best_weights=True, mode='min')\n",
    "early_stopping_auc = EarlyStopping(monitor='val_auc', patience=369, restore_best_weights=True, mode='max')\n",
    "#early_stopping_acc = EarlyStopping(monitor='accuracy', patience=420, restore_best_weights=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "# Train the model with tqdm loading bar\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    validation_data=(X_test_scaled, y_test),\n",
    "    epochs=1000,\n",
    "    batch_size=420,\n",
    "    callbacks=[early_stopping_loss, early_stopping_auc, TqdmCallback(verbose=1)],\n",
    "    verbose=0  # Set verbose to 0 to suppress the default progress bar\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the training data\n",
    "train_loss, train_accuracy, train_precision, train_recall, train_auc = model.evaluate(X_train_scaled, y_train, verbose=0)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy, test_precision, test_recall, test_auc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "print(f\"Training Evaluation:\\nLoss: {train_loss}\\nAccuracy: {train_accuracy}\\nPrecision: {train_precision}\\nRecall: {train_recall}\\nAUC: {train_auc}\\n\")\n",
    "print(model.evaluate(X_train_scaled, y_train, verbose=0))\n",
    "\n",
    "print(f\"Test Evaluation:\\nLoss: {test_loss}\\nAccuracy: {test_accuracy}\\nPrecision: {test_precision}\\nRecall: {test_recall}\\nAUC: {test_auc}\\n\")\n",
    "print(model.evaluate(X_test_scaled, y_test, verbose=0))\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the history of your metrics\n",
    "history_dict = history.history\n",
    "\n",
    "# List all your metrics\n",
    "metrics = ['loss', 'accuracy', 'precision', 'recall', 'auc']\n",
    "\n",
    "# Plot each metric in a separate subplot\n",
    "fig, axs = plt.subplots(5, 1, figsize=(10, 20))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    # Plot training metric\n",
    "    axs[i].plot(history_dict[metric], label='train')\n",
    "    # Plot validation metric\n",
    "    axs[i].plot(history_dict['val_'+metric], label='val')\n",
    "    # Set title, x and y labels\n",
    "    axs[i].set_title('Model '+ metric)\n",
    "    axs[i].set_xlabel('epoch')\n",
    "    axs[i].set_ylabel(metric)\n",
    "    # Show legend\n",
    "    axs[i].legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = r\"C:\\Users\\DE88342\\OneDrive - Grunenthal Group\\Desktop\\VSC - Py\\GRTend\\Models\\gender_model.keras\"\n",
    "\n",
    "# Save the model\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the full dataset\n",
    "\n",
    "full_df = pd.read_csv(r\"C:\\Users\\D\\OneDrive - Grunenthal Group\\Desktop\\VSC - Py\\GRTend\\Data\\data.csv\")\n",
    "\n",
    "#full_df.info()\n",
    "\n",
    "#seperate Gender and Null-Genders\n",
    "known_gender_df = full_df.dropna(subset=['Gender'])\n",
    "unknown_gender_df = full_df[full_df['Gender'].isna()]\n",
    "\n",
    "'''\n",
    "trying to impute missing values directly on a slice of the DataFrame, to avoid this warning, use the .loc method\n",
    "'''\n",
    "\n",
    "#impute missing numerical values\n",
    "unknown_gender_df.loc[:, num_cols] = imputer.transform(unknown_gender_df[num_cols]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical columns\n",
    "encoded_cols_unknown = encoder.transform(unknown_gender_df[cat_cols])\n",
    "encoded_cols_unknown = pd.DataFrame(encoded_cols_unknown, columns=encoder.get_feature_names_out(cat_cols)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of unknown_gender_df\n",
    "unknown_gender_df = unknown_gender_df[num_cols].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Concatenate numerical and encoded categorical columns\n",
    "unknown_gender_df = pd.concat([unknown_gender_df[num_cols], encoded_cols_unknown], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "unknown_gender_df_scaled = scaler.transform(unknown_gender_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to predict the 'Gender' for the rows with unknown 'Gender'\n",
    "predicted_gender = model.predict(unknown_gender_df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the missing 'Gender' values in the original dataset with the predicted values\n",
    "full_df.loc[full_df['Gender'].isna(), 'Gender'] = label_encoder.inverse_transform(predicted_gender.round().astype(int).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.drop('Job Function', axis=1, inplace=True)\n",
    "full_df.drop('Employment Details Termination Year', axis=1, inplace=True)\n",
    "full_df.to_csv(final_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv(final_dataset_path)\n",
    "\n",
    "'''\n",
    "to exclude the unrated data we use:\n",
    "'''\n",
    "\n",
    "final_df = final_df[final_df['rating_num'] != 0]\n",
    "\n",
    "#final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "#creating scatter plot in 3d for age tenure and rating\n",
    "\n",
    "fig = px.scatter_3d(final_df, x = 'Age', y = 'Tenure', z = 'rating_num',\n",
    "                    color = 'rating_num',\n",
    "                    color_continuous_scale = 'Viridis',\n",
    "                    title = 'Age, Tenure and Rating',\n",
    "                    labels = {'rating_num': 'Rating'})\n",
    "fig.update_layout(coloraxis_colorbar=dict(title='Rating'))\n",
    "\n",
    "fig.show()\n",
    "\n",
    "'''\n",
    "INFO: 1: Unsatisfactory, 2: Developing, 3: Performing, 4: Exceeding, 5: Outstanding\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "relevant_columns = ['Age', 'Tenure', 'FTE', 'rating_num', 'Gender']\n",
    "relevant_df = final_df[relevant_columns]\n",
    "relevant_df['Gender'] = relevant_df['Gender'].apply(lambda x: 1 if x == 'Male' else 0)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr = relevant_df.corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix of Relevant Features')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
