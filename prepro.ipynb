{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "'''LIBRARY-IMPORTS'''\n",
    "#####################\n",
    "\n",
    "#Importing all necessary libraries\n",
    "import pandas as pd #used for data manipluation\n",
    "import numpy as np #provides support for numerical operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "'''DATA-LOADING & MANIPULATION'''\n",
    "#################################\n",
    "\n",
    "#read the data from the csv files and store it in a dataframe, also add a column for the year (of rating, for composite key and general overview)\n",
    "df18 = pd.read_csv(r\"C:\\Users\\D\\OneDrive - Grunenthal Group\\Desktop\\VSC - Py\\GRTend\\Data\\report_Erik_EC_Data_18201231.csv\", delimiter= \",\", encoding  = 'ISO-8859-1', low_memory = False)\n",
    "df19 = pd.read_csv(r\"C:\\Users\\D\\OneDrive - Grunenthal Group\\Desktop\\VSC - Py\\GRTend\\Data\\report_Erik_EC_Data_20181231.csv\", delimiter= \",\", encoding  = 'ISO-8859-1', low_memory = False)\n",
    "df20 = pd.read_csv(r\"C:\\Users\\D\\OneDrive - Grunenthal Group\\Desktop\\VSC - Py\\GRTend\\Data\\report_Erik_EC_Data_20181231.csv\", delimiter= \",\", encoding  = 'ISO-8859-1', low_memory = False)\n",
    "df21 = pd.read_csv(r\"C:\\Users\\D\\OneDrive - Grunenthal Group\\Desktop\\VSC - Py\\GRTend\\Data\\report_Erik_EC_Data_20181231.csv\", delimiter= \",\", encoding  = 'ISO-8859-1', low_memory = False)\n",
    "df22 = pd.read_csv(r\"C:\\Users\\D\\OneDrive - Grunenthal Group\\Desktop\\VSC - Py\\GRTend\\Data\\report_Erik_EC_Data_20181231.csv\", delimiter= \",\", encoding  = 'ISO-8859-1', low_memory = False)\n",
    "\n",
    "df18['Year'] = 2018\n",
    "df19['Year'] = 2019\n",
    "df20['Year'] = 2020\n",
    "df21['Year'] = 2021\n",
    "df22['Year'] = 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define common columns and pass only those columns to the dataframe\n",
    "common_columns = [\n",
    "    'User/Employee ID', 'Username', 'Direct Reports', 'In current position since',\n",
    "    'Date Of Birth', 'Country Of Birth', '1. Citizenship', '2. Citizenship',\n",
    "    'Gender', 'Employee Status', 'Position', 'Position since', 'Position Level',\n",
    "    'Legal Entity', 'Country (LE)', 'Global Function', 'Employee Classification',\n",
    "    'New Job since', 'FTE', 'Job Function', 'Job Title', 'Employment Details Hire Date',\n",
    "    'Employment Details Original Start Date', 'Employment Details Calculation Start Date',\n",
    "    'Employment Details Termination Date', 'Year'\n",
    "]\n",
    "\n",
    "df18 = df18[common_columns]\n",
    "df19 = df19[common_columns]\n",
    "df20 = df20[common_columns]\n",
    "df21 = df21[common_columns]\n",
    "df22 = df22[common_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define date columns and transform them to datetime\n",
    "date_columns = ['In current position since', 'Date Of Birth', 'Employment Details Hire Date',\n",
    "                'Employment Details Original Start Date', 'Employment Details Calculation Start Date',\n",
    "                'Employment Details Termination Date', 'Position since', 'New Job since']\n",
    "\n",
    "for col in date_columns:\n",
    "    df18[col] = pd.to_datetime(df18[col], dayfirst= True, errors= 'coerce')\n",
    "    df19[col] = pd.to_datetime(df19[col], dayfirst= True, errors= 'coerce')\n",
    "    df20[col] = pd.to_datetime(df20[col], dayfirst= True, errors= 'coerce')\n",
    "    df21[col] = pd.to_datetime(df21[col], dayfirst= True, errors= 'coerce')\n",
    "    df22[col] = pd.to_datetime(df22[col], dayfirst= True, errors= 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define file paths for the output files\n",
    "dataset_file_path = r\"C:\\Users\\D\\OneDrive - Grunenthal Group\\Desktop\\VSC - Py\\GRTend\\Data\\data.csv\" #for the final dataset\n",
    "gender_file_path = r\"C:\\Users\\D\\OneDrive - Grunenthal Group\\Desktop\\VSC - Py\\GRTend\\Data\\gender_data.csv\" #for gender imputation/ model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a single dataframe for the employees, check DataFrame shape\n",
    "employee_df = pd.concat([df18, df19, df20, df21, df22], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load csv and create dataframe for ratings, check DataFrame shape and create 'Year' for composite key and general overview\n",
    "ratings_df = pd.read_csv(r'C:\\Users\\D\\OneDrive - Grunenthal Group\\Desktop\\VSC - Py\\GRTend\\Data\\_Ratings_ready to use.csv', delimiter= ';', encoding  = 'ISO-8859-1', low_memory = False)\n",
    "\n",
    "ratings_df.rename(columns={'username': 'Username'}, inplace=True)\n",
    "\n",
    "ratings_df['start-date'] = pd.to_datetime(ratings_df['start-date'], errors= 'coerce')\n",
    "\n",
    "ratings_df['Year'] = ratings_df['start-date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create composite key in ratings_df and employee_df and merge to Dataset\n",
    "\n",
    "employee_df['composite_key'] = employee_df['Username'] + '_' + employee_df['Year'].astype(str)\n",
    "\n",
    "ratings_df['composite_key'] = ratings_df['Username'] + '_' + ratings_df['Year'].astype(str)\n",
    "\n",
    "dataset = pd.merge(employee_df, ratings_df, on = 'composite_key', how = 'outer')\n",
    "\n",
    "dataset = dataset.dropna(subset=['rating'])\n",
    "\n",
    "print(\"Shape of merged 'employee' DataFrame: \", employee_df.shape)\n",
    "print(\"Shape of 'Ratings' DataFrame: \", ratings_df.shape)\n",
    "print(\"the shape of the 'Dataset' is: \", dataset.shape)\n",
    "\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "'''TESTS/ FUNCTIONS/ HELPERS'''\n",
    "###############################\n",
    "\n",
    "#check for outliers in numerical columns with the use of IQR (Interquartile Range/ Interquartilsabstand (deskripitve Statistik))\n",
    "def calculate_outliers(dataset, column):\n",
    "    Q1 = dataset[column].quantile(0.25)\n",
    "    Q3 = dataset[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return dataset[(dataset[column] < lower_bound) | (dataset[column] > upper_bound)]\n",
    "\n",
    "\n",
    "#define function to calculate modified z-score for outlier detection\n",
    "def calculate_modified_z_score(dataset, column):\n",
    "    median = dataset[column].median()\n",
    "    median_absolute_deviation = np.median(np.abs(dataset[column] - median))\n",
    "    modified_z_scores = 0.6745 * (dataset[column] - median) / median_absolute_deviation\n",
    "    return dataset[np.abs(modified_z_scores) > 3.5]\n",
    "\n",
    "def impute_legal_entity(dataset):\n",
    "    country_mapping = {\n",
    "        'DE': 'Germany', 'AT': 'Austria', 'CH': 'Switzerland', 'BE': 'Belgium', 'NL': 'Netherlands', 'LU': 'Luxembourg',\n",
    "        'GB': 'United Kingdom', 'FR': 'France', 'IT': 'Italy', 'ES': 'Spain', 'PT': 'Portugal', 'GR': 'Greece',\n",
    "        'PA': 'Panama', 'US': 'United States', 'CA': 'Canada', 'MX': 'Mexico', 'BR': 'Brazil', 'AR': 'Argentina',\n",
    "        'CL': 'Chile', 'PE': 'Peru', 'CO': 'Colombia', 'VE': 'Venezuela', 'EC': 'Ecuador', 'UY': 'Uruguay',\n",
    "        'NO': 'Norway', 'SE': 'Sweden', 'FI': 'Finland', 'DK': 'Denmark', 'PL': 'Poland', 'CZ': 'Czech Republic',\n",
    "        'IE': 'Ireland', 'HU': 'Hungary', 'RO': 'Romania', 'RU': 'Russia', 'TR': 'Turkey',\n",
    "    }\n",
    "\n",
    "    missing_legal_entity = dataset['Legal Entity'].isnull()\n",
    "\n",
    "    dataset.loc[missing_legal_entity, 'Legal Entity'] = dataset.loc[missing_legal_entity, 'Country (LE)'].apply(\n",
    "        lambda x: f\"GRT_{country_mapping.get(x, 'Unknown').upper()}\"\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "'''FEATURE ENGINEERING''' #stands for Feature Creation, handling missing values, encoding, scaling, extracting info, feature selection, feature extraction (reducing dimensionality)\n",
    "#########################\n",
    "\n",
    "#impute ['Employee Status'] with 'Active' if the Employee isnt terminated, else with 'Passive - termination'\n",
    "status_null_mask = dataset['Employee Status'].isnull()\n",
    "termination_null_mask = dataset['Employment Details Termination Date'].isnull()\n",
    "dataset.loc[status_null_mask & termination_null_mask, 'Employee Status'] = 'Active'\n",
    "dataset.loc[status_null_mask & ~termination_null_mask, 'Employee Status'] = 'Passive - termination'\n",
    "\n",
    "#impute missing values for ['Employee Classification'] with 'Employee' (most common value and statiscally likely)\n",
    "dataset['Employee Classification'].fillna('Employee', inplace = True)\n",
    "\n",
    "#impute ['Country (LE)'] and ['1. Citizenship'] - vice versa depending what we know\n",
    "dataset['Country (LE)'] = dataset['Country (LE)'].fillna(dataset['1. Citizenship'])\n",
    "dataset['1. Citizenship'] = dataset['1. Citizenship'].fillna(dataset['Country (LE)'])\n",
    "\n",
    "#impute ['Country Of Birth'] with ['1. Citizenship'] if missing (most likely)\n",
    "dataset['Country Of Birth'] = dataset['Country Of Birth'].fillna(dataset['1. Citizenship'])\n",
    "\n",
    "#create ['Native'] column to check if the employee is native to the country of the legal entity\n",
    "dataset['Native'] = np.where(dataset['1. Citizenship'] == dataset['Country (LE)'], True, False)\n",
    "\n",
    "#impute ['Legal Entity'] with mapping of ['Country (LE)'] and ['Legal Entity']\n",
    "dataset = impute_legal_entity(dataset)\n",
    "\n",
    "#create ['Birth year'] column to extract the year of birth from the date of birth\n",
    "dataset['Birth year'] = dataset['Date Of Birth'].dt.year\n",
    "\n",
    "#create ['In current position since'] column to extract the year of the current position start date\n",
    "dataset['In current position since'] = dataset['In current position since'].dt.year\n",
    "\n",
    "#create ['New Job since'] column to extract the year of the new job start date\n",
    "dataset['New Job since'] = dataset['New Job since'].dt.year\n",
    "\n",
    "#create ['Employment Details Hire Date'] column to extract the year of the hire date\n",
    "dataset['Employment Details Hire Year'] = dataset['Employment Details Hire Date'].dt.year\n",
    "\n",
    "#create ['Employment Details Termination Date'] column to extract the year of the termination date\n",
    "dataset['Employment Details Termination Year'] = dataset['Employment Details Termination Date'].dt.year\n",
    "\n",
    "#create ['Employment Details Calculation Start Date'] column to extract the year of the calculation start date\n",
    "dataset['Employment Details Calculation Start Year'] = dataset['Employment Details Calculation Start Date'].dt.year\n",
    "\n",
    "#create ['Employment Details Original Start Year'] column to extract the year of the original start date\n",
    "dataset['Employment Details Original Start Year'] = dataset['Employment Details Original Start Date'].dt.year\n",
    "\n",
    "#create ['Position since'] column to extract the year of the position start date\n",
    "dataset['Position since'] = dataset['Position since'].dt.year\n",
    "\n",
    "#catch outliers in the ['Birth year'] column, impute wrong values with the median\n",
    "birth_year_median = dataset.loc[dataset['Birth year'] > 1899, 'Birth year'].median()\n",
    "dataset.loc[dataset['Birth year'] <= 1899, 'Birth year'] = birth_year_median\n",
    "\n",
    "#create ['Start year'] column to extract the year of the original start date\n",
    "dataset['Start year'] = dataset['Employment Details Original Start Date'].dt.year\n",
    "\n",
    "#create ['Age'] column to calculate the age of the employee at the time of the rating\n",
    "dataset['Age'] = dataset['Year_x'] - dataset['Birth year']\n",
    "\n",
    "#catch outliers in ['Age'] column and impute with mean\n",
    "age_mean = dataset.loc[(dataset['Age'] >= 16) & (dataset['Age'] < 124), 'Age'].mean()\n",
    "dataset.loc[dataset['Age'] <= 16, 'Age'] = age_mean\n",
    "\n",
    "#create ['Tenure'] column to calculate the tenure of the employee at the time of the rating (actives)\n",
    "dataset['Tenure'] = dataset['Year_x'] - dataset['Start year']\n",
    "\n",
    "#create ['Tenure'] column to calculate the tenure of the employee at the time of the rating (terminated)\n",
    "dataset['Tenure'] = np.where(dataset['Employment Details Termination Date'].notnull(),\n",
    "                        (dataset['Employment Details Termination Date'] - dataset['Employment Details Original Start Date']).dt.days / 365,\n",
    "                        dataset['Tenure'])\n",
    "\n",
    "#prepare for creation of ['Age Group'] column\n",
    "bins = [0, 20, 30, 40, 50, 60, np.inf]\n",
    "labels = ['<20', '20-30', '30-40', '40-50', '50-60', '60+']\n",
    "\n",
    "#create ['Age Group'] column to categorize the age of the employees\n",
    "dataset['Age Group'] = pd.cut(dataset['Age'], bins = bins, labels = labels, right = False)\n",
    "\n",
    "#catch outliers in ['Tenure'] column, impute wrong values with the mean\n",
    "tenure_mean = dataset.loc[dataset['Tenure'] >= 0, 'Tenure'].mean()\n",
    "dataset.loc[dataset['Tenure'] <= 0, 'Tenure'] = tenure_mean\n",
    "\n",
    "#create ['Age at start'] column to calculate the age of the employee at the time of the original start date\n",
    "dataset['Age at start'] = dataset['Start year'] - dataset['Birth year']\n",
    "\n",
    "#catch outliers in ['Age at start'] and impute with mean\n",
    "startage_mean = dataset.loc[dataset['Age at start'] >= 16, 'Age at start'].mean()\n",
    "dataset.loc[dataset['Age at start'] <= 16, 'Age at start'] = startage_mean\n",
    "\n",
    "#create mode date to fill ['Position since'] missing values and impute with mode\n",
    "dataset['Position since'].fillna(dataset['Position since'].mode()[0], inplace=True)\n",
    "\n",
    "#define []'rating'] mapping, map ratings to numerical values\n",
    "rating_mapping = {\n",
    "    'Unrated': 0, 'Unsatisfactory': 1, 'Developing': 2, 'Performing': 3, 'Exceeding': 4, 'Outstanding': 5\n",
    "}\n",
    "\n",
    "dataset['rating_num'] = dataset['rating'].map(rating_mapping)\n",
    "\n",
    "#lets try imputing ['FTE'] with 1.0000 instead (mode didnt work, mean not accurate enough, 1.0000 close enough)\n",
    "dataset['FTE'].fillna(1.0000, inplace = True)\n",
    "\n",
    "#check and impute negative ['Direct Reports'], we impute negative ones with 0 till we know more\n",
    "dataset.loc[dataset['Direct Reports'] < 0, 'Direct Reports'] = 0\n",
    "\n",
    "#rename ['Year_y'] to ['Year']\n",
    "dataset.rename(columns={'Year_y': 'Year'}, inplace=True)\n",
    "\n",
    "#describe to check the distribution of numerical features\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Country (LE)'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_gender = dataset[dataset['Gender'].isnull()]\n",
    "no_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicates and remove such\n",
    "dataset.duplicated().sum()\n",
    "\n",
    "dataset = dataset.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "'''SENSITIVE DATA & REMOVAL OF UNNECESSARY COLUMNS'''\n",
    "#####################################################\n",
    "\n",
    "dataset = dataset.drop(['User/Employee ID', 'Username_x', 'Date Of Birth', 'Username_y', '^UserId',\n",
    "                        'In current position since', 'Employment Details Calculation Start Date',\n",
    "                        '2. Citizenship', 'Employment Details Termination Date', 'Employment Details Original Start Date',\n",
    "                        'Employment Details Hire Date', 'Position', 'Position Level', 'New Job since', 'Global Function',\n",
    "                        'Job Title', 'start-date', 'end-date', 'Year_x', '^AssignmentId', 'composite_key',\n",
    "                        'Employment Details Calculation Start Year'], axis='columns')\n",
    "\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final check for outliers in the datase\n",
    "#store outliers in a dictionary\n",
    "outliers = {}\n",
    "\n",
    "#check for numericallity and attach outliers to the dictionary\n",
    "for column in dataset.columns:\n",
    "    if dataset[column].dtype in ['int64', 'float64']:\n",
    "        outlier_df = calculate_modified_z_score(dataset, column)\n",
    "        outliers[column] = len(outlier_df)\n",
    "\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''save the dataset twice, once for gender imputation and once for the final dataset'''\n",
    "\n",
    "dataset.to_csv(gender_file_path, index = False)\n",
    "dataset.to_csv(dataset_file_path, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
